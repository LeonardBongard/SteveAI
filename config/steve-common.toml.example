[ai]
    # AI provider to use
    # Options: 'groq' (FASTEST, FREE), 'openai', 'gemini', or 'ollama' (self-hosted)
    provider = "groq"

[openai]
    # Your OpenAI API key
    # Get your API key from: https://platform.openai.com/api-keys
    apiKey = "your-openai-api-key-here"
    
    # Using GPT-3.5-turbo (much cheaper than GPT-4 for testing)
    model = "gpt-3.5-turbo"
    
    # Maximum tokens per API request
    maxTokens = 1000
    
    # Temperature (0.0-2.0, lower is more deterministic)
    temperature = 0.7

[ollama]
    # Ollama server base URL (for self-hosted LLMs)
    # Default is localhost. Change to your server's IP/domain if running remotely
    baseUrl = "http://127.0.0.1:11434"
    
    # Model to use (must be available in your Ollama instance)
    # Popular options: llama3.1:8b, mistral, codellama
    model = "llama3.1:8b"
    
    # API key (optional, only needed if Ollama is behind a reverse proxy with auth)
    apiKey = ""

[behavior]
    # Ticks between action checks (20 ticks = 1 second)
    actionTickDelay = 20
    
    # Allow Steves to respond in chat
    enableChatResponses = true
    
    # Maximum number of Steves that can be active simultaneously
    maxActiveSteves = 10

